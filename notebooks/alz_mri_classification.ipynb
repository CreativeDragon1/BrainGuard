{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e49fc95",
   "metadata": {},
   "source": [
    "# MRI Image Classification for Alzheimer's Detection\n",
    "Intermediate-level template using PyTorch (ResNet50 + custom CNN) with Grad-CAM explainability. Follow the cells to load data, train, evaluate, and visualize attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0dd6dd",
   "metadata": {},
   "source": [
    "## 1) Import Required Libraries\n",
    "Core imports for data handling, modeling, metrics, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f59afb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75a8903",
   "metadata": {},
   "source": [
    "## 2) Load and Explore MRI Dataset\n",
    "Point the paths to your MRI data (PNG/JPG or NIfTI). Inspect shapes, pixel ranges, and class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090c9ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set your data paths\n",
    "DATA_DIR = Path('../data')  # adjust to actual dataset\n",
    "IMG_EXT = ('.png', '.jpg', '.jpeg')\n",
    "NIFTI_EXT = ('.nii', '.nii.gz')\n",
    "\n",
    "# Example: load file list and simple EDA\n",
    "image_paths = []\n",
    "labels = []  # map to {class_name: index}\n",
    "\n",
    "# Placeholder: populate image_paths and labels here\n",
    "print(f\"Found {len(image_paths)} samples\")\n",
    "\n",
    "# Quick class balance check\n",
    "import collections\n",
    "counter = collections.Counter(labels)\n",
    "print('Class distribution:', counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff88c8c",
   "metadata": {},
   "source": [
    "## 3) Preprocess and Normalize Images\n",
    "Resize to 224x224, normalize, and add augmentation (rotation, flip). Handle NIfTI by taking a middle slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0991ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.preprocessing import normalize_image\n",
    "\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, paths, labels, train=True):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.train = train\n",
    "        self.base_transform = T.Compose([\n",
    "            T.Resize((224, 224)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485], std=[0.229])\n",
    "        ])\n",
    "        self.aug_transform = T.Compose([\n",
    "            T.RandomRotation(15),\n",
    "            T.RandomHorizontalFlip(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def load_image(self, path):\n",
    "        path = str(path)\n",
    "        if path.endswith(NIFTI_EXT):\n",
    "            vol = nib.load(path).get_fdata()\n",
    "            mid = vol.shape[2] // 2\n",
    "            arr = vol[:, :, mid]\n",
    "            arr = normalize_image(arr)\n",
    "            arr = (arr * 255).astype(np.uint8)\n",
    "            img = Image.fromarray(arr).convert('L')\n",
    "        else:\n",
    "            img = Image.open(path).convert('L')\n",
    "        return img\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.load_image(self.paths[idx])\n",
    "        if self.train:\n",
    "            img = self.aug_transform(img)\n",
    "        img = self.base_transform(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aafcde2",
   "metadata": {},
   "source": [
    "## 4) Split Data into Train, Validation, and Test Sets\n",
    "Use 70/15/15 split, keeping class balance if possible (stratify when you build `paths/labels`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d431ef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified split placeholder\n",
    "indices = np.arange(len(image_paths))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_end = int(0.7 * len(indices))\n",
    "val_end = int(0.85 * len(indices))\n",
    "\n",
    "train_idx = indices[:train_end]\n",
    "val_idx = indices[train_end:val_end]\n",
    "test_idx = indices[val_end:]\n",
    "\n",
    "train_ds = MRIDataset([image_paths[i] for i in train_idx], [labels[i] for i in train_idx], train=True)\n",
    "val_ds = MRIDataset([image_paths[i] for i in val_idx], [labels[i] for i in val_idx], train=False)\n",
    "test_ds = MRIDataset([image_paths[i] for i in test_idx], [labels[i] for i in test_idx], train=False)\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406aeb46",
   "metadata": {},
   "source": [
    "## 5) Build Convolutional Neural Network Model\n",
    "Use transfer learning (ResNet50 adapted to 1-channel) or the provided custom CNN baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de06a20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.cnn_model import ResNetModel, AlzheimersCNN\n",
    "\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "# Choose model: 'resnet' or 'cnn'\n",
    "model_choice = 'resnet'\n",
    "if model_choice == 'resnet':\n",
    "    model = ResNetModel(pretrained=True, num_classes=NUM_CLASSES)\n",
    "else:\n",
    "    model = AlzheimersCNN(num_classes=NUM_CLASSES)\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "print(model.__class__.__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcf6a79",
   "metadata": {},
   "source": [
    "## 6) Train the CNN Model\n",
    "Define loss, optimizer, and basic training loop. Monitor validation loss to catch overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da5e46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "\n",
    "def train_one_epoch(loader):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for imgs, lbls in loader:\n",
    "        imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(imgs)\n",
    "        loss = criterion(out, lbls)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        _, pred = out.max(1)\n",
    "        total += lbls.size(0)\n",
    "        correct += (pred == lbls).sum().item()\n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def validate(loader):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in loader:\n",
    "            imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)\n",
    "            out = model(imgs)\n",
    "            loss = criterion(out, lbls)\n",
    "            total_loss += loss.item()\n",
    "            _, pred = out.max(1)\n",
    "            total += lbls.size(0)\n",
    "            correct += (pred == lbls).sum().item()\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(lbls.cpu().numpy())\n",
    "    return total_loss / len(loader), correct / total, all_preds, all_labels\n",
    "\n",
    "EPOCHS = 5  # increase for real training\n",
    "for epoch in range(EPOCHS):\n",
    "    tr_loss, tr_acc = train_one_epoch(train_loader)\n",
    "    val_loss, val_acc, vp, vl = validate(val_loader)\n",
    "    scheduler.step(val_loss)\n",
    "    print(f\"Epoch {epoch+1}: train_loss={tr_loss:.4f} acc={tr_acc:.3f} | val_loss={val_loss:.4f} acc={val_acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781130e",
   "metadata": {},
   "source": [
    "## 7) Evaluate Model Performance\n",
    "Compute accuracy, precision, recall, F1, ROC-AUC, and confusion matrix on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7f7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbls in loader:\n",
    "            imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)\n",
    "            out = model(imgs)\n",
    "            probs = torch.softmax(out, dim=1)\n",
    "            _, pred = out.max(1)\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(lbls.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(f\"Test: acc={acc:.3f} prec={prec:.3f} rec={rec:.3f} f1={f1:.3f}\")\n",
    "    print('Confusion matrix:\\n', cm)\n",
    "\n",
    "evaluate(test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50631a3",
   "metadata": {},
   "source": [
    "## 8) Compare Multiple Models\n",
    "Swap between ResNet50, custom CNN, or other backbones (VGG, ResNet18). Track metrics side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdab6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_models = {\n",
    "    'resnet50': lambda: ResNetModel(pretrained=True, num_classes=NUM_CLASSES),\n",
    "    'custom_cnn': lambda: AlzheimersCNN(num_classes=NUM_CLASSES)\n",
    "    # Add 'resnet18' or VGG variants as needed\n",
    "}\n",
    "\n",
    "# TODO: loop over candidate_models, train briefly, and log metrics per model\n",
    "model_scores = {}\n",
    "for name, builder in candidate_models.items():\n",
    "    print(f\"\\n[Candidate] {name}\")\n",
    "    # NOTE: for brevity, re-use loaders; for real runs, re-init model/optimizer each loop\n",
    "    model = builder().to(DEVICE)\n",
    "    # train/validate as above, store best val acc/F1\n",
    "    model_scores[name] = {'val_acc': None, 'val_f1': None}\n",
    "\n",
    "print('Model comparison (fill after running):', model_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c416978",
   "metadata": {},
   "source": [
    "## 9) Visualize Predictions and Feature Maps\n",
    "Plot correct vs incorrect predictions and inspect activation maps/Grad-CAM overlays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dc1600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.grad_cam import GradCAM\n",
    "\n",
    "def show_grad_cam(img_tensor, model, target_class=None):\n",
    "    grad_cam = GradCAM(model, target_layer='layer4')\n",
    "    cam = grad_cam.generate(img_tensor, target_class)\n",
    "    plt.figure(figsize=(6,3))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(img_tensor.cpu().squeeze(), cmap='gray')\n",
    "    plt.axis('off'); plt.title('Input')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(img_tensor.cpu().squeeze(), cmap='gray')\n",
    "    plt.imshow(cam, alpha=0.5)\n",
    "    plt.axis('off'); plt.title('Grad-CAM')\n",
    "    plt.show()\n",
    "\n",
    "# TODO: grab a batch from test_loader and visualize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84088a8e",
   "metadata": {},
   "source": [
    "## 10) Interpret Model Results\n",
    "Summarize findings, note biases/limitations, and pick the best model. Export metrics/figures for the report and model card."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
